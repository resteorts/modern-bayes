\documentclass{article}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{amssymb}
%\newcommand{\btheta}{\boldsymbol{\theta}}
%\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
%\newcommand{\distas}[1]{\mathbin{\overset{#1}{\kern\z@\sim}}}%
\input{custom2}





\begin{document}
\title{Homework 8}
\author{STA-360/602}
%\date{5 PM Friday November 5}
\date{}
\maketitle

Total points: 10 (reproducibility) + 10 (Q1) = 20 points. 
\textbf{Q2 is worth up to 8 points of extra credit on this assignment to help anyone that has had a difficult time in the course and also to help you prepare for the final exam.} \\

\textbf{General instructions for homeworks}: Please follow the uploading file instructions according to the syllabus. You will give the commands to answer each question in its own code block, which will also produce plots that will be automatically embedded in the output file. Each answer must be supported by written statements as well as any code used. Your code must be completely reproducible and must compile. Syllabus: (https://github.com/resteorts/modern-bayes/blob/master/syllabus/syllabus-sta602-spring19.pdf)

\textbf{Advice}: Start early on the homeworks and it is advised that you not wait until the day of. While the professor and the TA's check emails, they will be answered in the order they are received and last minute help will not be given unless we happen to be free.  

\textbf{Commenting code}
Code should be commented. See the Google style guide for questions regarding commenting or how to write 
code \url{https://google.github.io/styleguide/Rguide.xml}. No late homework's will be accepted.

Please look over the homework before lab this week. TA's will answer questions on the homework this week regarding these two 
problems below. I recommend that you work through them as much as possible before lab this week. 

\begin{enumerate}
\item \emph{Lab component} (10 points) Please complete \textbf{Lab 10}, parts c and d which correspond with linear regression, which can be found here: \url{https://github.com/resteorts/modern-bayes/blob/master/labs/10-linear-regression/11-linear-regression_v2.pdf}.
It is highly recommend that you work through parts (a) and (b) on your own and derive these as these are excellent practice exercises for the exam. You can check your own work on this. 
\begin{enumerate}
\item[c)] (5 points) Complete lab 10, part c.
\item[d)] (5 points) Complete lab 10, part d.
\end{enumerate}

\newpage

\item \textbf{Extra Credit} (8 points) \emph{Multivariate Methods} 
This problem is optional for the homework, but it is worth extra credit. It is also a type of problem that is reasonable to appear on the final exam, and it's similar to what we did in class regarding the multivariate normal distribution. 

Because this problem is extra credit, the TA Team will not answer questions on this except for clarification questions. These should be sent privately. You should work on your own on this problem and not with others to help you prepare for the final examination. 

Consider the following hierarchical model:
\[ \bm{y}_i \mid \btheta,\bSigma \stackrel{i.i.d.}{\sim} MVN(\btheta_{d \times 1},\bSigma_{d \times d}), \quad i = 1, \cdots, n, \]
and independent priors $$\btheta_{d \times 1} \sim MVN(\bmu_{d \times 1},\bm{T}_{d \times d}), 
\qquad \bSigma_{d \times d} \sim \text{inverseWishart}(\nu, \Psi^{-1}_{d \times d}).$$
\begin{enumerate}
\item (1 point) Show that $(\btheta^T \bm{T}^{-1} \bmu)^T = \bmu^T \bm{T}^{-1} \btheta.$
%\item (1 point) Show that $(\bmu_{1 \times d}^T \bm{T_{d \times d}}^{-1}\btheta_{d \times 1})^T = \btheta^T \bm{T}^{-1} \bmu$.\\
\textit{Hint}: what's the transpose of a scalar?
\item (1 point) Use (a) to show that:
\[ p(\btheta) \propto e^{-\frac{1}{2}\left(\btheta^T\bm{T}^{-1}\btheta - 2\btheta^T \bm{T}^{-1} \bmu \right)} \]
\item (2 points) Use (b) to show that
$$ p(\btheta \mid \bSigma, \bm{y} )
\sim MVN\left\{\bmu^*(\bSigma) := {\bm{T}^*}(n \boldsymbol{\Sigma}^{-1}\bar{\bm{y}} + \bm{T}^{-1}\bmu), \bm{T}^* := ( n\boldsymbol{\Sigma}^{-1} + \bm{T}^{-1})^{-1}\right\}. $$
\item (2 points) Show that:
\[ \text{tr}(\Psi\boldsymbol{\Sigma}^{-1})+\sum_{i=1}^n (\bm{y}_i-\boldsymbol{\theta})^T \boldsymbol{\Sigma}^{-1}(\bm{y}_i-\boldsymbol{\theta}) = \text{tr}\left\{\left(\Psi + \sum_{i=1}^n (\bm{y}_i-\boldsymbol{\theta}) (\bm{y}_i-\boldsymbol{\theta})^T \right) \boldsymbol{\Sigma}^{-1}\right\}.\]
\textbf{Hint}: Recall that $\text{tr}(A)$ is the trace of matrix $A$ -- the sum of its diagonal entries. \textit{Lemma}: (i) for scalar $a$, $\text{tr}(a)=a$; (ii) $\text{tr}(ABC) = \text{tr}(BCA) = \text{tr}(CAB)$.
\item (2 points) Use (d) to show (step-by-step) that:
\[ [\bSigma|\btheta,\text{data}] \sim IW(\nu^*, \Psi^*(\btheta))\]
%where $\Psi^*(\btheta)$ and $\nu^*$ are in class notes. 
where $\nu^* = n + v$ and $\Psi^*(\btheta) = (\Psi + \sum_{i=1}^n (\bm{y}_i-\boldsymbol{\theta}) (\bm{y}_i-\boldsymbol{\theta})^T)^{-1} .$
\end{enumerate}
\end{enumerate}
\end{document}
