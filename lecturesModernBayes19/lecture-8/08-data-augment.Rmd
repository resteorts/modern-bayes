
---
title: "Module 8: Part III: Gibbs Sampling and Data Augmentation"
author: "Rebecca C. Steorts"
date: March 21, 2019
output: 
     beamer_presentation:
      includes: 
          in_header: custom2.tex
font-size: 8px
---

Agenda
===
- Two Component Mixture Model
- Latent Variable Allocation (Trick for Gibbs Sampling)
- Application to the Dutch Example


Data augmentation for auxiliary variables
===
\begin{itemize}
\item A commonly-used technique for designing MCMC samplers is to use \emph{data augmentation}, also known as \emph{auxiliary variables}.
\item Introduce variable(s) $Z$ that depends on the distribution of the existing variables in such a way that the
resulting conditional distributions, with $Z$ included, are easier to sample from and/or result in better mixing.
\item  $Z$'s are latent/hidden variables that are introduced for the purpose of simplifying/improving the sampler.
\end{itemize}

Idea: Create Z's and throw them away at the end! 
===
Goal: Sample from $p(x,y)$

Problem: We cannot sample from $p(x|y)$ and/or $p(y|x).$

Solution: 
\begin{enumerate}
\item Introduce a latent/hidden variable $Z$ such that 
$$p(x|y,z), p(y|x,z),  p(z|x,y)$$ are easy to sample from. 
\item Then construct a Gibbs sampler that will approximate samples from
$p(x,y,z)$ using the full conditionals above. 
\item The output of the three-stage Gibbs sampler is $(X,Y,Z).$
\end{enumerate}

But, our main interest is sampling from $p(x,y).$ Thus, we just throw away the $Z$'s and we will have samples $(X,Y)$ from $p(x,y)$.

Dutch Example
===
Consider a data set on the heights of 695 Dutch women and 562 Dutch men.  

\vskip 1em

Suppose we have the list of heights, but we don't know which data points
are from women and which are from men.  

Dutch Example 
===
From Figure \ref{figure:heights-combined} can we still infer the distribution of female heights and male heights?

\begin{figure}
  \begin{center}
    \includegraphics[width=1\textwidth]{examples/heights-combined}
    % Source: Original work by J. W. Miller.
  \end{center}
  \caption{Heights of Dutch women and men, combined.}
  \label{figure:heights-combined}
\end{figure}
Surprisingly, the answer is yes!

Dutch example
===
What's the magic trick?
\vskip 1em

The reason is that this is a two-component mixture of Normals,
and there is an (essentially) unique set of mixture parameters corresponding to any such distribution.

\vskip 1em

We'll get to details soon. Be patient!

Constructing a Gibbs sampler
===
To construct a Gibbs sampler for this situation:

\begin{itemize}
\item Common to introduce an auxiliary variable $Z_i$ for each data point,
indicating which mixture component it is drawn from.
\item  In this example, $Z_i$ indicates whether subject $i$ is female or male.
\item This results in a Gibbs sampler that is easy to derive/implement.
\end{itemize}

Two-component mixture model
===
Let's assume that both mixture components (female and male) have the same precision, say $\lambda$, and that $\lambda$ is fixed and known. Assume below that $a,b,m, \ell$ are known.

\vskip 1em

Then the usual two-component Normal mixture model is:
\begin{align}
  & X_1,\ldots, X_n \mid \mu,\pi\ \sim F(\mu,\pi)\\
      & \mu:=(\mu_0,\mu_1) \stackrel{iid}{\sim} \N(m,\ell^{-1})\\
        & \pi \sim \Beta(a,b),
\end{align}
Remark: $\pi$ is the prior probability that a subject comes from component 1.

where $F(\mu,\pi)$ is the distribution with p.d.f. 
$$ f(x|\mu,\pi) = (1-\pi)\N(x\mid \mu_0,\lambda^{-1}) + \pi
\N(x\mid \mu_1,\lambda^{-1}) $$
and $\mu=(\mu_0,\mu_1)$.


Likelihood
===
The likelihood is
\begin{align*}
    p(x_{1:n}|\mu,\pi) &= \prod_{i=1}^n f(x_i|\mu,\pi) \\
                       & = \prod_{i=1}^n \Big[ (1-\pi)\N(x_i\mid \mu_0,\lambda^{-1}) + \pi\N(x_i\mid \mu_1,\lambda^{-1}) \Big] 
\end{align*}
which is a complicated function of $\mu$ and $\pi$, making the posterior difficult to sample from directly.

Latent allocation variables to the rescue!
===
Define an equivalent model that includes latent ``allocation'' variables $Z_1,\ldots,Z_n.$
\vskip 1em

These indicate which mixture component each data point
comes from--that is, $Z_i$ indicates whether subject $i$ is female or male.
\begin{align}
 & X_i \mid \mu, Z \sim\N(\mu_{Z_i},\lambda^{-1}) \text{ independently for } i=1,\ldots,n.\\
 & Z_1,\ldots,Z_n \mid \mu,\pi\,\stackrel{iid}{\sim}\,\Bernoulli(\pi)\\
  & \mu= (\mu_0,\mu_1) \stackrel{iid}{\sim}\N(m,\ell^{-1})\\
    & \pi \sim \Beta(a,b)
\end{align}

How can we check that the latent allocation model is equivalent to our original model? 

Equivalence of both models
===
Recall
\begin{align*}
 & X_i \mid \mu, Z \sim\N(\mu_{Z_i},\lambda^{-1}) \text{ independently for } i=1,\ldots,n.\\
     & Z_1,\ldots,Z_n|\mu,\pi\,\stackrel{iid}{\sim}\,\Bernoulli(\pi)\\
    & \mu= (\mu_0,\mu_1) \stackrel{iid}{\sim} \N(m,\ell^{-1})\\
    & \pi \sim \Beta(a,b)
\end{align*}

<!-- This is equivalent to the model above, since -->
<!-- \begin{align} -->
<!--     &p(x_i|\mu,\pi) \\ -->
<!--     &= p(x_i|Z_i=0,\mu,\pi)\Pr(Z_i=0|\mu,\pi)  -->
<!--                     + p(x_i|Z_i=1,\mu,\pi)\Pr(Z_i=1|\mu,\pi) \\ -->
<!--             &= (1-\pi)\N(x_i|\mu_0,\lambda^{-1}) + \pi\N(x_i|\mu_1,\lambda^{-1})\\ -->
<!--             &= f(x_i|\mu,\pi), -->
<!-- \end{align} -->
<!-- and thus it induces the same distribution on $(x_{1:n},\mu,\pi)$. The latent model is considerably easier to work with, particularly for Gibbs sampling.  -->

Full conditionals
===
Recall
\begin{align*}
 & X_i \mid \mu, Z\sim \N(\mu_{Z_i},\lambda^{-1}) \text{ independently for } i=1,\ldots,n.\\
     & \textcolor{blue}{Z_1,\ldots,Z_n|\mu,\pi\,\stackrel{iid}{\sim}\,\Bernoulli(\pi)}\\
    & \mu= (\mu_0,\mu_1) \stackrel{iid}{\sim} \N(m,\ell^{-1})\\
    & \textcolor{blue}{\pi \sim \Beta(a,b)}
\end{align*}

\begin{itemize}
    \item ($\pi|\cdots$) Given $z$, $\pi$ is independent of everything else, so this reduces to a Beta--Bernoulli model, and we have
        $$ p(\pi|\mu,z,x) = p(\pi|z) =\Beta(\pi\mid a + n_1,\, b + n_0) $$
        where $n_k = \sum_{i=1}^n \I(z_i=k)$ for $k \in \{0, 1\}$.
        \end{itemize}
        
Full conditionals
===
Recall 
\begin{align*}
 & \textcolor{blue}{X_i\sim\N(\mu_{Z_i},\lambda^{-1}) \text{ independently for } i=1,\ldots,n.}\\
     & Z_1,\ldots,Z_n|\mu,\pi\,\stackrel{iid}{\sim}\,\Bernoulli(\pi)\\
    & \textcolor{blue}{\mu= (\mu_0,\mu_1) \stackrel{iid}{\sim} \N(m,\ell^{-1})}\\
    & \pi \sim \Beta(a,b)
\end{align*}

\begin{itemize}        
    \item ($\mu|\cdots$) Given $z$, we know which component each data point comes from. 
    \vskip 1em
    The model (conditionally on $z$) is just two
        independent Normal--Normal models, as we have seen before:
        \begin{align*}
            \bm\mu_0|\mu_1,x,z,\pi\, \sim \,\N(M_0,L_0^{-1})\\
            \bm\mu_1|\mu_0,x,z,\pi\, \sim \,\N(M_1,L_1^{-1})
        \end{align*}
        where for $k\in\{0,1\}$,
        \begin{align*}
        n_k &= \sum_{i=1}^n \I(z_i=k)\\
        L_k &=\ell + n_k\lambda\\
        M_k &=\frac{\ell m + \lambda\sum_{i:z_i=k} x_i}{\ell + n_k\lambda}.
        \end{align*}
        \end{itemize}
        
Full conditionals
===
\begin{itemize}
    \item ($z|\cdots$)
        \begin{align*}
            p(z|\mu,\pi,x)&\underset{z}{\propto}p(x,z,\pi,\mu)\underset{z}{\propto} p(x|z,\mu) p(z|\pi)\\
            & =\prod_{i = 1}^n \N(x_i|\mu_{z_i},\lambda^{-1})\Bernoulli(z_i|\pi)\\
            & =\prod_{i = 1}^n \Big(\pi\N(x_i|\mu_1,\lambda^{-1})\Big)^{z_i} \Big((1-\pi)\N(x_i|\mu_0,\lambda^{-1})\Big)^{1-z_i}\\
            & =\prod_{i = 1}^n \alpha_{i,1}^{z_i} \alpha_{i,0}^{1-z_i}\\
            &\underset{z}{\propto}\prod_{i = 1}^n\Bernoulli(z_i\mid \alpha_{i,1}/(\alpha_{i,0}+\alpha_{i,1}))
        \end{align*}
        where
        \begin{align*}
            \alpha_{i,0} & =(1-\pi)\N(x_i|\mu_0,\lambda^{-1})\\
            \alpha_{i,1} & =\pi\N(x_i|\mu_1,\lambda^{-1}).
        \end{align*}
\end{itemize}



My Factory Settings!
===

We initialize the sampler at the same settings that we did when we looked at this application before. Let's review them below. 

\begin{itemize}
\item $\lambda = 1/\sigma^2$ where $\sigma = 8$ cm ($\approx 3.1$ inches) ($\sigma$ = standard deviation of the subject heights within each component)
\item $a = 1$, $b = 1$ (Beta parameters, equivalent to prior ``sample size'' of 1 for each component)
\item $m = 175$ cm ($\approx 68.9$ inches) (mean of the prior on the component means)
\item $\ell = 1/s^2$ where $s = 15$ cm ($\approx 6$ inches) ($s$ = standard deviation of the prior on the component means)
\end{itemize}

My Factory Settings!
===

We initialize the sampler at the same settings that we did when we looked at this application before. Let's review them below. 

\begin{itemize}
\item $\pi = 1/2$ (equal probability for each component)
\item $z_1,\ldots,z_n$ sampled i.i.d.\ from $\Bernoulli(1/2)$ (initial assignment to components chosen uniformly at random)
\item $\mu_0 =\mu_1 = m$ (component means initialized to the mean of their prior)
\end{itemize}


Traceplots of the component means
===
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\textwidth]{examples/mix-mu_trace-a.png}\caption{Traceplots of the component means, $\mu_0$ and $\mu_1$. The blue component quickly settles to have a mean of around 168–170 cm and the other (red) to a mean of around 182–186 cm.}
\label{default}
\end{center}
\end{figure}

Recall we're not using the true assignments of subjects to components (we don’t know whether they are male or female). 
Note that this is fairly close to the sample averages: 168.0 cm (5 feet 6.1 inches) for females, and 181.4 cm (5 feet 11.4 inches) for males. 

Traceplots of the mixture weight, $\pi$
===    
\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{examples/mix-p_trace-a.png}
\caption{Traceplots of the mixture weight, $\pi$ (prior probability that a subject comes from component 1). }
\label{figure:mixb}
\end{center}  
\end{figure}

The traceplot of $\pi$ indicates that the sampler is exploring values of around 0.2 to 0.4—that is, the proportion of people coming from group 1 is around 0.2 to 0.4.

Looking at the actual labels (female and male), the empirical proportion of males is $562/(695 + 562) \approx 0.45,$ so this is 
slight off. 

How might we fix this in our model? 

Back to the model
===

Recall
\begin{align*}
 & X_i \mid \mu, Z \sim\N(\mu_{Z_i},\lambda^{-1}) \text{ independently for } i=1,\ldots,n.\\
     & Z_1,\ldots,Z_n|\mu,\pi\,\stackrel{iid}{\sim}\,\Bernoulli(\pi)\\
    & \mu= (\mu_0,\mu_1) \stackrel{iid}{\sim} \N(m,\ell^{-1})\\
    & \pi \sim \Beta(a,b),
\end{align*}

where $a,b,m,\ell$ are fixed.

Hint: Is it reasonable to think that we should have the same $\lambda$ for the data?
Consider $$\lambda = (\lambda_0, \lambda_1)$$

How would you proceed next in a Bayesian hierarchical manner? 


Histograms
===
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\textwidth]{examples/mix-histograms_at_last_sample-a.png}\caption{Histograms of the heights of subjects assigned to each component, according to $z_1,\ldots,z_n$, in a typical sample.}
\label{default}
\end{center}
\end{figure}


Question
===

Why are females assigned to component 0 and males assigned to component 1? 
Why not the other way around? Let's investigate this and see what happens if we look at another choice of starting values. 
    
Traceplots of the component means
===
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\textwidth]{examples/mix-mu_trace-a.png}
\includegraphics[width=\textwidth]{examples/mix-mu_trace-b.png}\caption{Traceplots of the component means, $\mu_0$ and $\mu_1$.}
\label{default}
\end{center}
\end{figure} 

Traceplots of the mixture weight, $\pi$
===    
\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{examples/mix-p_trace-a.png}
\includegraphics[width=\textwidth]{examples/mix-p_trace-b.png}
\caption{Traceplots of the mixture weight, $\pi$ (prior probability that a subject comes from component 1). }
\label{figure:mixb}
\end{center}  
\end{figure}

Histograms
===
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\textwidth]{examples/mix-histograms_at_last_sample-a.png}
\includegraphics[width=\textwidth]{examples/mix-histograms_at_last_sample-b.png}\caption{Histograms of the heights of subjects assigned to each component, according to $z_1,\ldots,z_n$, in a typical sample.}
\label{default}
\end{center}
\end{figure}


Caution: watch out for modes
===
Example illustrates a big thing that can go wrong with MCMC (although fortunately, in this case, the results are still valid if interpreted correctly). 
\begin{itemize}
\item Why are females assigned to component 0 and males assigned to component 1? Why not the other way around? 
\item In fact, the model is symmetric with respect to the two components, and thus the posterior is also symmetric. 
\item If we run the sampler multiple times (starting from the same initial values), sometimes it will settle on females as 0 and males as 1, and sometimes on females as 1 and males as 0 --- see Figure \ref{figure:mixb}. 
\item Roughly speaking, the posterior has two modes.
\item  If the sampler were behaving properly, it would move back and forth between these two modes.
\item But it doesn't---it gets stuck in one and stays there.
\end{itemize}

Takeaway from example
===
- This is a very common problem with mixture models. 
- Fortunately, however, in the case of mixture models, the results are still valid if we interpret them correctly. 
- Specifically, our inferences will be valid as long as we only consider quantities that are invariant with respect to permutations of the components (e.g. symmetry about the mean, others).
- To learn more about invariance and what quantities remain unchanged, see Theory of Point Estimation, Lehmann and Casella, Chapter 1 for a formal treatment. (For more advanced reading, see Chapter 3.) Feel free to see me in OH if you want to chat about this! 








