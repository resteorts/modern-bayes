
---
title: "Module 2: Introduction to Decision Theory"
author: "Rebecca C. Steorts"
output: 
     beamer_presentation:
      includes: 
          in_header: custom2.tex
font-size: 8px
---

Announcements
===

1. No late assignments on homeworks will be accepted.
2. Please make sure you are familar with the syllabus and course policies. 
3. If you do think that there was an error in grading your homework, please do submit a re-grade according to the policy on the syllabus. 
4. If you have any questions at all, please see me in OH. 
5. We will go over material leading up to exam I soon! 

Exercise
===

Suppose that I come across a paper on Bayesian analysis and it's the following paper: 

https://arxiv.org/abs/1409.0643

1. How would you verify the mathematical results are correct?
2. How would you verify that the figures/analysis is correct? 
3. What would easily convince you that the figures/analysis is correct? 



Solution
===

1. You would have to verify these. 
2. You could code up everything on your own. 
3. If all the results were reproducible through say a package in R, this would give me great confidence that the results in the paper are correct. 

https://cran.r-project.org/web/packages/blink/index.html

Takeaway: This is why reproducibility is a big part of this course and also a large part of your homework grade. 

Agenda
===
- What is decision theory?
- General setup
- Example of general set up
- Bayesian risk
- Frequentist Risk
- Integrated Risk
- An Application to Resource Allocation 

General setup
===
Assume an unknown state $S$ (a.k.a. the state of nature).
Assume 

- we receive an observation $x$, 
- we take an action $a$, and
- we incur a real-valued loss $\ell(S,a)$.

\begin{center}
\begin{tabular}{ c l }
$S$ & state (unknown) \\
$x$ & observation (known) \\
$a$ & action \\
$\ell(s,a)$ & loss
\end{tabular}
\end{center}

Example 
===

1. State: $S = \btheta$
2. Observation: $x = x_{1:n}$
3. Action: $a = \hat\theta$
4. Loss: $\ell(\theta,\hat\theta) = (\theta-\hat\theta)^2$ (quadratic loss, a.k.a. square loss)

\textcolor{red}{Question: Why do we consider quadratic loss over other loss functions?}


Bayesian approach
===

- $S$ is a random variable, 
- the distribution of $x$ depends on $S$, 
- and the optimal decision is to choose an action $a$ that minimizes the \term{posterior expected loss},
$$\rho(a,x) =\E(\ell(S,a)|x).$$

In other words, $\rho(a,x) =\sum_s \ell(s,a) p(s|x)$ if $S$ is a discrete random variable, while if $S$ is continuous then the sum is replaced by an integral.

Bayesian approach (continued)
===

1. A \term{decision procedure} $\delta$ is a systematic way of choosing actions $a$ based on observations $x$. Typically, this is a deterministic function $a=\delta(x)$ (but sometimes introducing some randomness into $a$ can be useful).
2. A \term{Bayes procedure} is a decision procedure that chooses an $a$ minimizing the posterior expected loss $\rho(a,x)$, for each $x$.
3. Note: Sometimes the loss is restricted to be nonnegative, to avoid certain pathologies. 

What is the optimal decision rule?
===

- Goal: Minimize the posterior risk
- First note that 
$$\ell(\theta,\hat\theta) =\theta^2 - 2\theta\hat\theta + \hat\theta^2$$
- It then follows that the \textbf{posterior loss} is
\begin{align*}
\rho(\hat\theta,x_{1:n})&=\E(\ell(\theta,\hat\theta)|x_{1:n}) =
\E((\theta-\hat\theta)^2|x_{1:n}) \\
&= 
\E(\theta^2 - 2\theta\hat\theta + \hat\theta^2|x_{1:n})\\
&=\E(\theta^2|x_{1:n}) - 2\hat\theta\E(\theta|x_{1:n}) +\hat\theta^2,
\end{align*}
which is a convex function of $\hat\theta$. 

What is the optimal decision rule?
===
We just showed that
$$\rho(\hat\theta,x_{1:n}) = \E(\theta^2|x_{1:n}) - 2\hat\theta\E(\theta|x_{1:n}) +\hat\theta^2$$
Setting the derivative with respect to $\hat\theta$ equal to $0$, and solving, we find that the minimum occurs at $\hat\theta = \E(\btheta|x_{1:n})$, \textbf{the posterior mean}. 

Let's walk through this derivation together. 

What is the optimal decision rule?
===
\begin{align*}
\frac{\partial \rho(\hat\theta,x_{1:n})}{\partial \hat{\theta}}
&= \frac{\partial \{\E(\theta^2|x_{1:n}) - 2\hat\theta\E(\theta|x_{1:n}) +\hat\theta^2\}}{\partial \hat{\theta}}
&= -2 \E(\theta|x_{1:n}) + 2 \hat{\theta}
\end{align*}
Now, let 
$$ -2 \E(\theta|x_{1:n}) + 2 \hat{\theta} = 0, $$
which implies that $$\hat{\theta} = \E(\theta|x_{1:n}).$$

Why is the solution unique?

Resource allocation for disease prediction
===

Suppose public health officials in a small city need to decide how much resources to devote toward prevention and treatment of a certain disease, but the fraction $\theta$ of infected individuals in the city is unknown.

Resource allocation for disease prediction (continued)
===

Suppose they allocate enough resources to accomodate a fraction $c$ of the population. Recall that $\theta$ is the fraction of the infected individuals in the city. 

- If $c$ is too large, there will be wasted resources, while if it is too small, preventable cases may occur and some individuals may go untreated. 
- After deliberation, they adopt the following loss function:
$$\ell(\theta,c) =\branch{|\theta-c|}{c\geq\theta}
                       {10|\theta-c|}{c<\theta.}$$
                       
Resource allocation for disease prediction (continued)
===                       
                       
- By considering data from other similar cities, they determine a prior $p(\theta)$. For simplicity, suppose $\btheta\sim\Beta(a,b)$ (i.e., $p(\theta) =\Beta(\theta|a,b)$), with $a=0.05$ and $b=1$.\footnote{We could certainly consider other choices of $a,b$ but we consider these choices for simplicity. You'll look at other choices in lab/homework.}

-  They conduct a survey assessing the disease status of $n=30$ individuals, $x_1,\ldots,x_n$. 

This is modeled as $X_1,\ldots,X_n \stackrel{iid}{\sim} \Bernoulli(\theta)$, which is reasonable if the individuals are uniformly sampled and the population is large. Suppose all but one are disease-free, i.e.,
    $\sum_{i=1}^n x_i = 1$.

The Bayes procedure
===
The **Bayes procedure** is to minimize the posterior expected loss
$$\rho(c,x) =\E(\ell(\btheta,c)|x) = \int \ell(\theta,c)p(\theta|x)d\theta $$
where $x = x_{1:n}$.

1.  We know $p(\theta|x)$ as an updated Beta, so we can numerically compute this integral for each $c$.
2. Figure \ref{figure:rho} shows $\rho(c,x)$ for our example.
3. The minimum occurs at $c\approx 0.08$, so under the
    assumptions above, this is the optimal amount of resources to allocate. 
4. How would one perform a sensitivity analysis of the prior assumptions?


Resource allocation for disease prediction in R
===
```{r, cache=TRUE}
# set seed 
set.seed(123)

# data
sum_x = 1
n = 30
# prior parameters
a = 0.05; b = 1
# posterior parameters
an = a + sum_x
bn = b + n - sum_x
th = seq(0,1,length.out = 100)
# writing the likelihood as a beta 
# a trick from module 1
like = dbeta(th, sum_x+1,n-sum_x+1)
prior = dbeta(th,a,b)
post = dbeta(th,sum_x+a,n-sum_x+b)
```

Likelihood, Prior, and Posterior
===
```{r, echo=FALSE}
plot(th, like, type = "l", ylab = "Density", 
     xlab = expression(theta), lty = 2, lwd = 3, 
     col = "green",ylim = c(0,21) )
lines(th, prior, lty = 3, lwd = 3, col= "red")
lines(th, post, lty=1, lwd = 3, col= "blue")
legend(0.6,10, c("Prior", "Likelihood", "Posterior"), lty=c(2,3,1), lwd=c(3,3,3), 
       col = c("red", "green", "blue"))
```

The loss function
===
```{r}
# compute the loss given theta and c 
loss_function = function(theta, c){
  if (c < theta){
    return(10*abs(theta - c))
  } else{
    return(abs(theta - c))
  }
}
```

\textcolor{red}{Where can I learn about functions/brush up on usage?}
https://resteorts.github.io/teach/bayes19

Posterior risk
===
```{r}
# compute the posterior risk given c 
# s is the number of random draws 
posterior_risk = function(c, s = 30000){
  # randow draws from posterior distribution
  # which is a beta with params an and bn
  theta = rbeta(s, an, bn)
  
  # calculating values of the loss times the posterior
  loss <- apply(as.matrix(theta),1,loss_function,c)
  # average values from the loss function (integral)
  risk = mean(loss)
}
```

Posterior Risk (continued)
===
```{r}
# a sequence of c in [0, 0.5]
c = seq(0, 0.5, by = 0.01)
post_risk <- apply(as.matrix(c),1,posterior_risk)
head(post_risk)
```

Posterior expected loss/posterior risk for disease prevelance
===
```{r}
# plot posterior risk against c 

pdf(file="posterior-risk.pdf")
plot(c, post_risk, type = 'l', col='blue', 
    lwd = 3, ylab ='p(c, x)' )
dev.off()
# minimum of posterior risk occurs at c = 0.08
(c[which.min(post_risk)])
```

Posterior expected loss/posterior risk for disease prevelance
===
\begin{figure}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{figures/posterior-risk}
    % Source: Original work by R.C. Steorts
  \end{center}
  \caption{}
 \label{figure:rho}
\end{figure}

Sensitivity Analysis
===

\textcolor{red}{Suppose now that $a = 0.05, 1, 0.05$ and $b = 1, 2, 10.$}

\textcolor{red}{If we have different prior, the posterior risk is minimized at different $c$ values. The optimal $c$ depends on not only the data, but also the prior setting.}

Posterior Risk Function (More Advanced)
===
```{r, cache=TRUE}
# compute the posterior risk given c 
# s is the number of random draws 
posterior_risk = function(c, a_prior, b_prior, 
                          sum_x, n, s = 30000){
  # randow draws from beta distribution 
  a_post = a_prior + sum_x
  b_post = b_prior + n - sum_x
  theta = rbeta(s, a_post, b_post)
  loss <- apply(as.matrix(theta),1,loss_function,c)
  # average values from the loss function
  risk = mean(loss)
}
```

Posterior Risk Function (More Advanced)
===
```{r, cache=TRUE}
# a sequence of c in [0, 0.5]
c = seq(0, 0.5, by = 0.01)
post_risk <- apply(as.matrix(c),1,
                   posterior_risk, a, b, sum_x, n)
head(post_risk)
```

Sensitivity Analysis
===
```{r, cache=TRUE}
# set prior
as = c(0.05, 1, 0.05); bs = c(1, 1, 10)
post_risk = matrix(NA, 3, length(c))

# for each pair of a and b, compute the posterior risks
for (i in 1:3){
  a_prior = as[i]
  b_prior = bs[i]

  # using the more advanced function 
  # of the posterior risk  
post_risk[i,] = apply(as.matrix(c), 1,
                      posterior_risk, a_prior,
                      b_prior, sum_x, n)
}
```

Plot
===
```{r}
plot(c, post_risk[1,], type = 'l', 
     col='blue', lty = 1, yaxt = "n", ylab = "p(c, x)")
par(new = T)
plot(c, post_risk[2,], type = 'l', 
     col='red', lty = 2, yaxt = "n", ylab = "")
par(new = T)
plot(c, post_risk[3,], type = 'l', 
     lty = 3, yaxt = "n", ylab = "")
legend("bottomright", lty = c(1,2,3), 
       col = c("blue", "red", "black"), 
       legend = c("a = 0.05 b = 1", 
                  "a = 1 b = 1", "a = 0.05 b = 5"))
```

Optimal resources (a,b vary)
===
\textcolor{red}{For $a = 0.05, 1, 0.05$ and $b = 1, 2, 10$ respectively,
the optimal value for c is:}

```{r}
(c[which.min(post_risk[1,])])
(c[which.min(post_risk[2,])])
(c[which.min(post_risk[3,])])
```


Plot
===
```{r, echo=FALSE}
plot(c, post_risk[1,], type = 'l', 
     col='blue', lty = 1, yaxt = "n", ylab = "p(c, x)")
par(new = T)
plot(c, post_risk[2,], type = 'l', 
     col='red', lty = 2, yaxt = "n", ylab = "")
par(new = T)
plot(c, post_risk[3,], type = 'l', 
     lty = 3, yaxt = "n", ylab = "")
legend("bottomright", lty = c(1,2,3), 
       col = c("blue", "red", "black"), 
       legend = c("a = 0.05 b = 1", 
                  "a = 1 b = 1", "a = 0.05 b = 5"))

```


Frequentist Risk
===
1. Consider a decision problem in which $S = \btheta$.
2. The \term{risk} (or \term{frequentist risk}) associated with a decision procedure $\delta$ is 
$$ R(\theta,\delta) = \E\big(\ell(\btheta,\delta(X))\mid\btheta =\theta\big),$$
where $X$ has distribution $p(x|\btheta)$.  In other words,
$$ R(\theta,\delta) = \int \ell(\theta,\delta(x))\,p(x|\theta)\,dx$$
if $X$ is continuous, while the integral is replaced with a sum if $X$ is discrete.

The integrated risk
===

The \term{integrated risk}  associated with $\delta(X)$ is
\begin{align}
r(\delta) &= \E(\ell(\btheta,\delta(X)) = \int R(\theta,\delta)\,p(\theta)\,d\theta \\
&= \int \int \ell(\theta,\delta(x))\,p(x|\theta)p(\theta)\,dx \,d\theta
\end{align}



Example: Resource allocation, revisited
===
1. The frequentist risk provides a useful way to compare decision procedures in a prior-free way.
2. In addition to the Bayes procedure or Bayes rule that we have considered earlier in the lecture, consider two other potential optimal decision rules: choosing $c = \bar x$ (sample mean) or $c=0.1$ (constant).\footnote{Recall: The Bayes rule minimizes the posterior risk with respect to the parameter of interest.}
3. \textcolor{red}{Remark: both the frequentist rules are looking an optimal estimator in a prior free way. (There are many other examples, but we'll just look at two simple cases.)}

Example: Resource allocation, revisited
===
3. Figure \ref{figure:procedures} shows each procedure as a function of $\sum x_i$, the observed number of diseased cases. For the prior we have chosen, the Bayes procedure always picks $c$ to be a little bigger than $\bar x$.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.85\textwidth]{figures/procedures.png}
    % Source: Original work by J. W. Miller.
  \end{center}
  \caption{Resources allocated $c$, as a function of the number of diseased individuals observed, $\sum x_i$, for the three different procedures.}
  \label{figure:procedures}
\end{figure}

Example: Resource allocation, revisited
===

4. Figure \ref{figure:risk} shows the risk $R(\theta,\delta)$ as a function of $\theta$ for each procedure. Smaller risk is better. (Recall that for each $\theta$, the risk is the expected loss, averaging over all possible data sets. The observed data doesn't factor into it at all.)

\begin{figure}
  \begin{center}
    \includegraphics[width=0.85\textwidth]{figures/risk.png}
    % Source: Original work by J. W. Miller.
  \end{center}
  \caption{Risk functions for the three different procedures.}
  \label{figure:risk}
\end{figure}

Example: Resource allocation, revisited
===

5. The constant procedure is fantastic when $\theta$ is near $0.1$, but gets very bad very quickly for larger $\theta$. The Bayes procedure is better than the sample mean for nearly all $\theta$'s. These curves reflect the usual situation---some procedures will work better for certain $\theta$'s and some will work better for others.
6. A decision procedure which is  \term{inadmissible} is one that is dominated everywhere.
That is, $\delta$ is \term{\textcolor{red}{admissible}} if there is no $\delta'$ such that $$R(\theta,\delta')\leq R(\theta,\delta)$$
for all $\theta$ and $R(\theta,\delta')< R(\theta,\delta)$ for at least one $\theta$. (A decision procedure that is not \term{inadmissible} is said to be \term{admissible}).
7. Bayes procedures are admissible under very general conditions.
8. Admissibility is nice to have, but it doesn't mean a procedure is necessarily good. Silly procedures can still be admissible---e.g., in this example, the constant procedure $c = 0.1$ is admissible too!

Example: Resource allocation, revisited
===

In lab and in your homework, you'll work on reproducing the plots from class today and exploring more about admissibility. 

To read more about this formally, please see Lehmann and Casella for a very formal treatment of this approach. 

\textcolor{red}{You may also find the PhD lecture notes helpful on this material, which are avaiable to you on the course webpage.}
https://resteorts.github.io/teach/bayes19

Takeaways
===

- In understanding an optimal decision rule, we first must have a parameter of interest ($\theta$) and define an optimal estimator ($\delta(X)$ or $\hat{\theta}$). 

- There are many ways to define a loss function. A few that we talked about were the 0-1, quadratic, and absolute value loss. 

- Next, we define several ways of finding an optimal decision rule. There were three that we considered. We considered minimizing the posterior risk (Bayes rule), the risk (frequentist risk), or the integrated risk. 

- Finally, we defined admissible/inadmissible rules. 




