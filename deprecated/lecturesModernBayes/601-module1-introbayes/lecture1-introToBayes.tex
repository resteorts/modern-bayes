\documentclass[mathserif]{beamer}

\setbeamertemplate{frametitle}[default][center]%Centers the frame title.
\setbeamertemplate{navigation symbols}{}%Removes navigation symbols.
\setbeamertemplate{footline}{\raisebox{5pt}{\makebox[\paperwidth]{\hfill\makebox[10pt]{\scriptsize\insertframenumber}}}}
\setbeamertemplate{caption}[numbered]

%\input{multi_symbols.tex}

\usepackage{float}
\floatstyle{boxed}
\newfloat{code}{tp}{code}
\floatname{code}{Code Example}
\input{multi_symbols}
%\usepackage{fontspec}
%\setmainfont{Tahoma}

%\newcommand{\lam}{\lambda}
%\newcommand{\bmu}{\bm{\mu}}
%%\newcommand{\bx}{\ensuremath{\mathbf{X}}}
%\newcommand{\X}{\ensuremath{\mathbf{x}}}
%\newcommand{\w}{\ensuremath{\mathbf{w}}}
%\newcommand{\h}{\ensuremath{\mathbf{h}}}
%\newcommand{\V}{\ensuremath{\mathbf{v}}}
%\newcommand{\cov}{\text{Cov}}
%\newcommand{\var{\text{Var}}}

%\DeclareMathOperator{\var}{Var}
%\DeclareMathOperator{\cov}{Cov}

%\newcommand{\indep}{\rotatebox{90}{\ensuremath{\models}}}
%\newcommand{\notindep}{\not\hspace{-.05in}\indep}

\usepackage{graphicx} %The mode "LaTeX => PDF" allows the following formats: .jpg  .png  .pdf  .mps
\graphicspath{{./PresentationPictures/}} %Where the figures folder is located
\usepackage{listings}
\usepackage{media9}
\usepackage{movie15}
\addmediapath{./Movies/}

\newcommand{\beginbackup}{
   \newcounter{framenumbervorappendix}
   \setcounter{framenumbervorappendix}{\value{framenumber}}
}
\newcommand{\backupend}{
   \addtocounter{framenumbervorappendix}{-\value{framenumber}}
   \addtocounter{framenumber}{\value{framenumbervorappendix}} 
}


%\usepackage{algorithm2e}
\usepackage[ruled,lined]{algorithm2e}
\def\algorithmautorefname{Algorithm}
\SetKwIF{If}{ElseIf}{Else}{if}{then}{else if}{else}{endif}
%\usepackage{times}
%\usepackage[tbtags]{amsmath}
%\usepackage{amssymb}
\usepackage{amsfonts}
%\usepackage{slfortheorems}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage[small]{caption}
%\usepackage[square]{natbib}
%\newcommand{\newblock}{}
%\bibpunct{(}{)}{;}{a}{}{,}
%\bibliographystyle{ims}
%\usepackage[letterpaper]{geometry}
\usepackage{color}
\setlength{\parindent}{0pt}

\usepackage{natbib}
\bibpunct{(}{)}{;}{a}{}{,}
%\usepackage{hyperref}

\hypersetup{
  colorlinks,
  citecolor=green,
  urlcolor=blue,
  linkcolor=white
}

%\usepackage{zref-savepos}
%
%\newcounter{restofframe}
%\newsavebox{\restofframebox}
%\newlength{\mylowermargin}
%\setlength{\mylowermargin}{2pt}
%
%\newenvironment{restofframe}{%
%    \par%\centering
%    \stepcounter{restofframe}%
%    \zsavepos{restofframe-\arabic{restofframe}-begin}%
%    \begin{lrbox}{\restofframebox}%
%}{%
%    \end{lrbox}%
%    \setkeys{Gin}{keepaspectratio}%
%    \raisebox{\dimexpr-\height+\ht\strutbox\relax}[0pt][0pt]{%
%    \resizebox*{!}{\dimexpr\zposy{restofframe-\arabic{restofframe}-begin}sp-\zposy{restofframe-\arabic{restofframe}-end}sp-\mylowermargin\relax}%
%        {\usebox{\restofframebox}}%
%    }%
%    \vskip0pt plus 1filll\relax
%    \mbox{\zsavepos{restofframe-\arabic{restofframe}-end}}%
%    \par
%}


\usepackage{tikz}
\usetikzlibrary{arrows}

%\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tkz-berge}
\usetikzlibrary{fit,shapes}

\usepackage{calc}
%%
%% The tikz package is used for doing the actual drawing.
%\usepackage{tikz}
%%
%% In order to be able to put arrowheads in the middle of directed edges, we need an extra library.
\usetikzlibrary{decorations.markings}
%%
%% The next line says how the "vertex" style of nodes should look: drawn as small circles.
\tikzstyle{vertex}=[circle, draw, inner sep=0pt, minimum size=6pt]
%%
%% Next, we make a \vertex command as a shorthand in place of \node[vertex} to get that style.
\newcommand{\vertex}{\node[vertex]}
%%
%% Finally, we declare a "counter", which is what LaTeX calls an integer variable, for use in
%% the calculations of angles for evenly spacing vertices in circular arrangements.
\newcounter{Angle}

\newtheoremstyle{example}
{\topsep} % space above
{\topsep} % space below
{} % body font
{} % indent
{\bf} % head font
{:} % punctuation between head and body
{0.5em} % space after head
{} % manually specify head
%{\thmname{#1}\thmnumber{ #2}\thmnote{:#3}} % manually specify head

\theoremstyle{example}
\newtheorem{ex}{Example}[section]

\newtheoremstyle{definition}
{\topsep} % space above
{\topsep} % space below
{} % body font
{} % indent
{\sc} % head font
{:} % punctuation between head and body
{0.5em} % space after head
{} % manually specify head
%{\thmname{#1}\thmnumber{ #2}\thmnote{:#3}} % manually specify head

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]

\theoremstyle{rem}
\newtheorem{rem}{Remark}[section]

\newtheoremstyle{theorem}
{\topsep} % space above
{\topsep} % space below
{} % body font
{} % indent
{\sc} % head font
{:} % punctuation between head and body
{0.5em} % space after head
{} % manually specify head
%{\thmname{#1}\thmnumber{ #2}\thmnote{:#3}} % manually specify head

\theoremstyle{theorm}
\newtheorem{thm}{Theorem}[section]



%%%to add in new counter for slides in beamer

%\setbeamertemplate{footline}{
%  \leavevmode%
%  \hbox{%
%  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
%    \usebeamerfont{author in head/foot}\insertshortauthor~~(\insertshortinstitute)
%  \end{beamercolorbox}%
%  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
%    \usebeamerfont{title in head/foot}\insertshorttitle
%  \end{beamercolorbox}%
%  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
%    \usebeamerfont{date in head/foot}\insertshortdate{}\hspace*{2em}
%    \insertframenumber{} \hspace*{2ex} % hier hat's sich geŠndert
%  \end{beamercolorbox}}%
%  \vskip0pt%
%}



%%%%%

\newcommand*\oldmacro{}
\let\oldmacro\insertshortauthor
\renewcommand*\insertshortauthor{
  \leftskip=.3cm
\insertframenumber\,/\,\inserttotalframenumber\hfill\oldmacro}




%\excludecomment{notbeamer}
%\includecomment{beamer}



\title{Intro to Bayesian Methods}
\author{Rebecca C. Steorts \\ Bayesian Methods and Modern Statistics: STA 360/601}
\date{Lecture 1}

\begin{document}

\maketitle





%\pagestyle{plain} for plain doc
%\excludecomment{notreport}
%\includecomment{report}

%\include{cover}

%\tableofcontents
%\baselineskip 24pt
%\setlength{\parskip}{0.3cm}
%\setlength{\parindent}{0cm}
%\setcounter{chapter}{0}


%\chapter{Introduction}
%\emph{There are three kinds of lies: lies, damned lies and statistics.}\\
%---Mark Twain
%\newpage
%\frame{
%\center
%\textbf{Intro to Bayesian concepts}
%\vspace*{2em}
%
%}

\frame{
\begin{itemize}
\item  \href{https://stat.duke.edu/~rcs46/bayes.html}{Course Webpage}
\item \href{https://stat.duke.edu/~rcs46/syllabus_duke_bayes_601.pdf}{Syllabus}
\item  \href{https://tobi.oetiker.ch/lshort/lshort.pdf}{LaTeX reference manual}
\item  \href{https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf}{R markdown reference manual}
\begin{itemize}
\end{itemize}
\item Please come to office hours for all questions. 
\begin{itemize}
\item Office hours are not a review period if you cannot come to class. 
\end{itemize}
\item \href{https://groups.google.com/forum/\#!forum/bbayes}{Join Google group}
\item Graded on Labs/HWs, Exams.
\begin{itemize}
\item Labs/HWs and Exams .R markdown format (it must compile).
\item Nothing late will be accepted. 
\item You're lowest homework will be dropped. 
\end{itemize}
\item Announcements: Emails or in class. 
\item All your lab/homework assignments will be uploaded to Sakai. 
\item How to reach me and TAs -- email or Google. 
\end{itemize}
}





\frame{
\frametitle{Expectations}
\begin{itemize}
\item Class is optional but you are expected to know everything covered in lecture. 
\item Not everything will always be on the slides. 
\item 2 Exams: in class, timed. Closed book, closed notes. (Dates are on the syllabus). 
\item There are NO make up exams. 
\item Late assignments will not be accepted. Don't ask. 
\item Final exam: during finals week. 
\item You should be reading the book as we go through the material in class.
\end{itemize}
}

\frame{
\frametitle{Expectations for Homework}
\begin{itemize}
\item Your write ups should be clearly written.
\item Proofs: show all details. 
\item Data analysis: clearly explain. 
\item For data analysis questions, don't just turn in code. 
\item Code must be well documented. 
\item Code style: https://google.github.io/styleguide/Rguide.xml
\item For all homeworks, can use Markdown or LaTex. You must include all files
that lead to your solutions (this includes code)! 
\end{itemize}
}

\frame{
\frametitle{Things available to you! }
\begin{itemize}
\item Come to office hours. We want to help you learn! 
\item Supplementary reading to go with the notes by yours truly. (Beware of typos). 
\begin{itemize}
\item \href{https://stat.duke.edu/~rcs46/books/babybayes-master.pdf}{Undergrad level notes}
\item \href{https://stat.duke.edu/~rcs46/books/bayes_manuscripts.pdf}{PhD level notes}
\item Example form of write up in .Rmd on Sakai (Module 0). 
\item You should have your homeworks graded and returned within one week by the TA's! 
\end{itemize}

\end{itemize}
}

\frame{
\begin{itemize}
\item Why should we learn about Bayesian concepts?
\item Natural if thinking about unknown parameters as random.
\item They naturally give a full distribution when we perform an update.
\item We automatically get uncertainty quantification. 
\item Drawbacks: They can be slow and inconsistent. 
\end{itemize}
}

\frame{
\frametitle{Record linkage}

Record linkage is the process of merging together noisy databases to remove duplicate entries. 

}

\frame{
\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.35]{pics/google_jsm}
%\caption{default}
%\label{default}
\end{center}
\end{figure}



}

\frame{
\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{pics/steve_1}
%\caption{default}
%\label{fig:figure1}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{pics/blank}
%\caption{default}
%\label{fig:figure2}
\end{minipage}
\end{figure}
}

\frame{
\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{pics/steve_1}
%\caption{default}
%\label{fig:figure1}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{pics/steve_2}
%\caption{default}
%\label{fig:figure2}
\end{minipage}
\end{figure}
\pause

These are clearly not the \emph{same} Steve Fienberg!


}

\frame{
\frametitle{Syrian Civil War}
\vspace*{-0.75em}
\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.35]{pics/syrian}
%\caption{default}
%\label{default}
\end{center}
\end{figure}

}

\frame{
\frametitle{Bayesian Model }
\small
\begin{itemize}
\item Define
$
\alpha_\ell(w)=
%\frac{1}{N}\sum_{i=1}^k\sum_{j=1}^{n_i}I(X_{ij\ell}=w).%=
\text{relative frequency of $w$ in data for field $\ell$}.
$
\pause
\item $G_\ell$: empirical distribution for field $\ell.$
%\item $W \sim G_\ell, \implies \forall w$,
%$
%P(W=w)=\alpha_\ell(w).
%$
\pause
\item $W\sim F_\ell(w_0)$: $\textcolor{black}{P(W=w)\propto\alpha_\ell(w)\,\exp\!\left[-c\,d(w,w_0)\right],}$
where $d(\cdot,\cdot)$ is a string metric and $c>0$.
\end{itemize}
\pause
\vspace*{-1em}
\begin{align*}
X_{ij\ell}\mid \lambda_{ij},\,Y_{\lambda_{ij}\ell},\,z_{ij\ell}\;&\sim\begin{cases}\delta(Y_{\lambda_{ij}\ell})&\text{ if }z_{ij\ell}=0\\F_\ell(Y_{\lambda_{ij}\ell})&\text{ if }z_{ij\ell}=1\text{ and field }\ell\text{ is string-valued}\\G_\ell&\text{ if }z_{ij\ell}=1\text{ and field }\ell\text{ is categorical}\end{cases}\\
%&\qquad\text{for each }i\in\{1,\ldots,k\},\; j\in\{1,\ldots,n_i\},\; \ell\in\{1,\ldots,p_s+p_c\},\\
%&\qquad\text{with everything independent},\\
Y_{j'\ell}\;&\sim G_\ell\\
z_{ij\ell}\mid\beta_{i\ell}\;&\sim \text{Bernoulli}(\beta_{i\ell})\\
\beta_{i\ell}\;&\sim\text{Beta}(a,b)\\
\lambda_{ij}\;&\sim\text{DiscreteUniform}(1,\ldots,N_{\max}),\quad
\text{ where }N_{\max}=\sum_{i=1}^k n_i.
\end{align*}
%with everything independent of everything else.


}



\frame{

The model I showed you is very complicated.
\vskip 1em
This course will give you an intro to Bayesian models and methods. 
\vskip 1em
Often Bayesian models are hard to work with, so we'll learn about approximations.
\vskip 1em The above record linkage problem is one that needs such an approximation. 

}

%\section{Motivations}
%\frame{
%
%Suppose we have some noisy data. How can we recover the underlying structure of the data?
%}

%\frame{
%\begin{figure}[htbp]
%\begin{center}
%\includegraphics[scale=0.7]{license}
%%\caption{ Image of a car license plate }
%\label{default}
%\end{center}
%\end{figure}
%}
%
%\frame{
%\begin{figure}[htbp]
%\begin{center}
%\includegraphics[scale=0.7]{lake}
%\caption{ Satellite image of the lake of Menteith, Scotland }
%\label{default}
%\end{center}
%\end{figure}
%}
%
%\frame{
%\begin{figure}[htbp]
%\begin{center}
%\includegraphics[scale=0.4]{segmentation}
%\caption{ Dataset Menteith: (top) the observed image  and (bottom) 
%Segmented image}
%\label{default}
%\end{center}
%\end{figure}
%}
%
%
%




%\frame{
%\emph{The adjective (Bayesian) appeared again sporadically in philosophy and statistics journals for the next 20 years, but the use of ÒfrequentistÓ to describe statistical methods
% gained currency in the 1950s only after ÒBayesianÓ came into common usage, and then
%it was used by Bayesians to describe non-Bayesian methods.}
%
%-Steve Fienberg, Bayesian Analysis, (2007)
%}


%\frame{
%\begin{figure}[htbp]
%\begin{minipage}[b]{0.45\linewidth}
%\centering
%\includegraphics[width=\textwidth]{fisher.jpg}
%\caption{R.A. Fisher}
%\label{fig:figure1}
%\end{minipage}
%\hspace{0.5cm}
%\begin{minipage}[b]{0.45\linewidth}
%\centering
%\includegraphics[width=0.7\textwidth]{lindley.jpg}
%\caption{Dennis Lindley}
%\label{fig:figure2}
%\end{minipage}
%\end{figure}
%}
%
%
%\frame{
%\begin{figure}[htbp]
%\begin{minipage}[b]{0.45\linewidth}
%\centering
%\includegraphics[width=\textwidth]{larry.jpg}
%\caption{freq turned Bayes turned freq}
%\label{fig:figure1}
%\end{minipage}
%\hspace{0.5cm}
%\begin{minipage}[b]{0.45\linewidth}
%\centering
%\includegraphics[width=0.7\textwidth]{bill.jpg}
%\caption{avid freq}
%\label{fig:figure2}
%\end{minipage}
%\end{figure}
%}
%
%\frame{
%
%\begin{figure}[htbp]
%\begin{minipage}[b]{0.4\linewidth}
%\centering
%\includegraphics[width=\textwidth]{jay.png}
%\caption{avid bayesian}
%\label{fig:figure1}
%\end{minipage}
%\hspace{0.5cm}
%\begin{minipage}[b]{0.45\linewidth}
%\centering
%\includegraphics[width=\textwidth]{cosma2.jpg}
%\caption{avid freq (it's cosma)}
%\label{fig:figure2}
%\end{minipage}
%\end{figure}
%\newpage
%}

\section{Bayes' Theorem}

\frame{

\begin{itemize}
\item ``Bayesian'' traces its origin to the 18th century and English Reverend Thomas Bayes, who along with Pierre-Simon Laplace discovered what we now call ``Bayes' Theorem".
\end{itemize}
\pause
\begin{align}
\label{bayes}
p(\theta|x) = \frac{p(x|\theta)p(\theta)}{p(x)} \propto p(x|\theta)p(\theta).
\end{align}
\pause
%The proportionality $\propto$ in Eq. (\ref{bayes}) signifies that the $1/p(x)$ factor is constant and may be ignored when viewing $p(\theta|x)$ as a function of $\theta$.
We can decompose Bayes' Theorem into three principal terms:
\begin{eqnarray*}
\pause
p(\theta|x) & \qquad& \text{posterior}\\
\pause
p(x|\theta) & \qquad& \text{likelihood}\\
\pause
p(\theta) & \qquad& \text{prior}
\end{eqnarray*}

%Bayes' Theorem provides a general recipe for updating prior beliefs about an unknown parameter~$\theta$ based on observing some data~$x$.
}

\subsection{Polling Example}

\frame{
\frametitle{Polling Example 2012}
Let's apply this to a real example! We're interested in the proportion of people that approve of President Obama in PA. 
\pause
\begin{itemize}
\item We take a random sample of 10 people in PA and find that 6 approve of President Obama. 
\pause
\item The national approval rating (Zogby poll) of President Obama in mid-December was 45\%. We'll assume that in PA his approval rating is approximately 50\%.
\pause
\item Based on this prior information, we'll use a Beta prior for $\theta$ and we'll choose $a$ and $b.$ (Won't get into this here). 
\pause
\item We can plot the prior and likelihood distributions in \texttt{R} and then see how the two mix to form the posterior distribution. 
\end{itemize}

}



\frame{
\begin{center}
\includegraphics[width=.9\textwidth]{pics/obama_prior}
\end{center}
}

\frame{

\begin{center}
\includegraphics[width=.9\textwidth]{pics/obama_likprior}
\end{center}
}


\frame{
\begin{center}
\includegraphics[width=.9\textwidth]{pics/obama_all}
\end{center}
}

\section{History}

%\section{Advantages of Bayesian Methods}
\frame{


The basic philosophical difference between the frequentist and Bayesian paradigms is that 
\begin{itemize}
\item Bayesians treat an unknown parameter~$\theta$ as \emph{random}.
%and use probability to quantify their uncertainty about it.
\pause  
\item Frequentists treat $\theta$ as unknown but \emph{fixed}.
%and they therefore believe that probability statements about $\theta$ are useless.  
\end{itemize}

%This fundamental disagreement leads to entirely different ways to handle statistical problems, even problems that might at first seem very basic.
}



\section{Only the Likelihood Matters!}

\frame{
\frametitle{Stopping Rule}
Let $\tth$ be the probability of a particular coin landing on
heads, and suppose we want to test the hypotheses
\pause
$$H_0 : \tth = 1/2,\qquad H_1 : \tth > 1/2$$
at a significance level of $\alpha=0.05$.
Suppose we observe the following sequence of flips:
$$\text{heads, heads, heads, heads, heads, \textbf{tails}}\qquad\text{(5 heads, 1 tails)}$$
\pause
\begin{itemize}
\item To perform a frequentist hypothesis test, we must define a random variable to describe the data.  
\pause
\item The proper way to do this depends on exactly which of the following two experiments was actually performed:
\end{itemize}
}

\frame{
\begin{itemize}
\item Suppose the experiment is ``\textbf{Flip six times and record the results.}''  
\pause
\begin{itemize}
\item $X$ counts the number of heads, and $X\sim\text{Binomial}(6,\theta)$.  
\item The observed data was $x=5$, and the p-value of our hypothesis test is
\end{itemize}
\begin{eqnarray*}
\pause
\text{p-value}&=&P_{\theta=1/2}(X\ge5)\\
&=&P_{\theta=1/2}(X=5)+P_{\theta=1/2}(X=6)\\
\pause
&=&\frac{6}{64}+\frac{1}{64}=\frac{7}{64}=0.109375>0.05.
\end{eqnarray*}
\pause
\textbf{So we fail to reject} $H_0$ at $\alpha=0.05$.
\end{itemize}
}

\frame{

\begin{itemize}
\item Suppose now the experiment is ``\textbf{Flip until we get tails.}'' \\ 
\pause
\begin{itemize}
\item $X$ counts the number of the flip on which the first tails occurs, and $X\sim\text{Geometric}(1-\theta)$. 
\item  The observed data was $x=6$, and the p-value of our hypothesis test is
\end{itemize}
\begin{eqnarray*}
\text{p-value}&=&P_{\theta=1/2}(X\ge6)\\
\pause
&=&1-P_{\theta=1/2}(X<6)\\
\pause
&=&1-\sum_{x=1}^5P_{\theta=1/2}(X=x)\\
\pause
&=&1-\left(\frac{1}{2}+\frac{1}{4}+\frac{1}{8}+\frac{1}{16}+\frac{1}{32}\right)
=\frac{1}{32}=0.03125<0.05.
\end{eqnarray*}
\textbf{So we reject} $H_0$ at $\alpha=0.05$.
\end{itemize}

}
\frame{
\begin{itemize}
\item The conclusions differ, which seems strikes \emph{some people} as absurd.  
\pause
\item P-values aren't close---one is 3.5 times as large as the other.  
\pause
\item The result our hypothesis test depends on whether we would have stopped flipping if we had gotten a tails sooner. 
\pause
\item The tests are dependent on what we call the \emph{stopping rule}. 
%\item The frequentist approach requires us to specify what we would have done had the data been something that we already know it wasn't.
\end{itemize}
}

%\newpage
\frame{
\begin{itemize}
\item The likelihood for the actual value of $x$ that was observed is the same for both experiments (up to a constant):
$$p(x|\theta)\propto\theta^5(1-\theta).$$
\pause
\item A Bayesian approach would take the data into account only through this likelihood.
\pause
\item  This would  provide the same answers regardless of which experiment was being performed.
\end{itemize} 
\vspace*{1em}

%This is because the Bayesian analysis is independent of the stopping rule (think about how to show this at home). 

The Bayesian analysis is independent of the stopping rule since it only depends on the likelihood (show this at home!). 
}
%
%\begin{ex}
%Suppose we want to test whether the voltage~$\theta$ across some electrical component differs from 9~V, based on noisy readings of this voltage from a voltmeter.  Suppose the data is as follows:
%$$9.7,\; 9.4,\; 9.8,\; 8.7,\; 8.6$$
%A frequentist might assume that the voltage readings $X_i$ are iid from some $N(\theta,\sigma^2)$ distribution, which would lead to a basic one-sample $t$-test.
%
%However, the frequentist is then presented with an additional piece of information: 
%\textbf{The voltmeter used for the experiment only went up to 10~V, and any readings that might have otherwise been higher are instead truncated to that value.}  Notice that \emph{none of the voltages in the data are 10~V}.  In other words, we already know that the 10~V limit was completely irrelevant for the data we actually observed.
%
%Nevertheless, a frequentist must now redo the analysis and could perhaps obtain a different conclusion, because the 10~V limit changes the distribution of the observations under the null hypothesis.  Like in the last example, the frequentist results change based on what would have happened had the data been something that we already know it wasn't.
%\end{ex}
%
%The problems in Examples~1.1~and~1.2 arise from the way the frequentist paradigm forces itself to interpret probability.  Another familiar aspect of this problem is the awkward definition of ``confidence'' in frequentist confidence intervals.  
%
%\textbf{The most natural interpretation} of a 95\% confidence interval~$(L,U)$---that there is a 95\% chance that the parameter is between $L$ and $U$---\textbf{is dead wrong from the frequentist point of view.}  
%
%Instead, the notion of ``confidence'' must be interpreted in terms of repeating the experiment a large number of times (in principle, an infinite number), and no probabilistic statement can be made about \emph{this particualar} confidence interval computed from the data we actually observed.
%
%

\section{Hierarchical Bayesian Models}

\frame{
\frametitle{Hierarchical Bayesian Models}
In a hierarchical Bayesian model, rather than specifying the prior distribution as a single function, we specify it as a hierarchy. 
}
%Thus, on the unknown parameter of interest, say $\theta,$ we put a prior. On any other unknown \emph{hyperparameters} of the model that are given, we also specify priors for these. We write
\frame{
\frametitle{Hierarchical Bayesian Models}
\begin{align*}
X|\theta &\sim f(x|\theta) \\
\Theta|\gamma &\sim \pi(\theta|\gamma)\\
\Gamma &\sim \phi(\gamma),
\end{align*}
where we assume that $\phi(\gamma)$ is known and not dependent on any other unknown \emph{hyperparameters}. 
}

%Note that we can continue this hierarchical modeling and add more stages to the model, however note that doing so adds more complexity to the model (and possibly as we will see may result in a posterior that we cannot compute without the aid of numerical integration or MCMC, which we will cover in detail in a later chapter). 

\frame{
\frametitle{Conjugate Distributions}
Let $F$ be the class of sampling distributions $p(y|\theta).$ 
\pause
\begin{itemize}
\item Then let $P$ denote the class of prior distributions on~$\theta.$ 
\pause
\item Then $P$ is said to be conjugate to $F$ if for every $p(\theta) \in P$ and $p(y|\theta) \in F,$ $p(\theta \mid y) \in P.$
\end{itemize}
 \textbf{Simple definition}:
A family of priors such that, upon being multiplied by the likelihood,
yields a posterior in the same family. 
}

\begin{frame}

\frametitle{Beta-Binomial}
If $X|\tth$ is distributed as $\text{binomial} (n, \tth)$, then a conjugate prior
is the beta family of distributions, where we can show that the posterior is
\pause
\begin{eqnarray*}
%\begin{align*}
\pi(\theta|x) &\propto &  p(x|\theta)p(\theta)\\
\pause
& \propto  &
\binom{n}{x} \theta^x (1-\theta)^{n-x}
\frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}
\theta^{a-1}(1-\theta)^{b-1} \\
\pause
&\propto &
 \theta^x (1-\theta)^{n-x} 
\theta^{a-1}(1-\theta)^{b-1} \\
\pause
& \propto &
 \theta^{x + a -1} (1-\theta)^{n-x + b-1} \implies
%\end{align*}
\end{eqnarray*}
\pause
$$\theta|x  \sim \text{Beta}(x+a,n- x +b).$$
\end{frame}


%\frame{
%
%\frametitle{Gamma-InverseGamma}
%%
%%\begin{ex}
%%\label{gamma}
%\begin{eqnarray*}
%X|\alpha,\beta &\sim& \text{Gamma}(\alpha,\beta),\; \alpha \;\text{known},\;\beta\; \text{unknown}\\
%\beta &\sim& \text{IG}(a,b).
%\end{eqnarray*}
%\pause
%%\end{ex}
%Calculate the posterior distribution of $\beta|x.$
%\begin{eqnarray*}
%p(\beta|x) &\propto& \frac{1}{\Gamma{(\alpha)}\beta^\alpha}x^{\alpha-1}e^{-x/\beta}\times
%\frac{b^a}{\Gamma{(a)}}\beta^{-a-1}e^{-b/\beta}\\
%\pause
%&\propto& \frac{1}{\beta^\alpha}e^{-x/\beta}
%\beta^{-a-1}e^{-b/\beta}\\
%\pause
%&=&\beta^{-\alpha-a-1}e^{-(x+b)/\beta} \implies
%\end{eqnarray*}
%%Notice that this looks like an Inverse Gamma distribution with parameters $\alpha + a$ and $x+b.$ Thus,
%$$\beta|x \sim IG(\alpha + a, x+b).$$
%}

\end{document}