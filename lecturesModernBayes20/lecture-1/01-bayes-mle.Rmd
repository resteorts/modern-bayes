
---
title: "Maximum Likelihood Estimation and Bayesian Statistics"
author: "Rebecca C. Steorts"
output: 
     beamer_presentation:
      includes: 
          in_header: custom2.tex
font-size: 8px
---



Agenda
===
- Maximum Likelihood Esimation
- Unbiased Estimators


Traditional inference
===

You are given **data** $X$ and there is an **unknown parameter** you wish to estimate **$\theta$**

How would you estimate $\theta$?

- Find an unbiased estimator of $\theta$. 
- Find the maximum likelihood estimate (MLE) of $\theta$ by looking at the likelihood of the data. 
- Suppose that $\hat{\theta}$ estimates $\theta.$ 

Note: $\hat{\theta}$ may depend on the data $x_{1:n} = x_1, \ldots x_n.$

Unbiased Estimator
===
Recall that $\hat{\theta}$ is an **unbiased estimator** of $\theta$ if 

\begin{equation}
E[\hat{\theta}] = \theta.
\end{equation}.

Maximum  Likelihood Estimation
===

For each sample point $x_{1:n},$ let $\hat{\theta}$ be a parameter value at which $p(x_{1:n} \mid \theta)$ attains it's maximum as a function of $\theta,$ with $x_{1:n}$ held fixed. 

A **maximum likelihood esimator** (MLE) of the parameter $\theta$ based on a sample $x_{1:n}$ is $\hat{\theta}.$

Finding the MLE
===

The solution to the MLE are the possible candidates ($\theta$) that solve 

\begin{equation}
\label{eqn:mle}
\frac{\partial p(x_{1:n} \mid \theta)}{\partial \theta} = 0.
\end{equation}

The solution to equation \ref{eqn:mle} are only **possible candidates** for the MLE since the first derivative being 0 is a **necessary condition** for a maximum but not a sufficient one. 

Our job is to find a global maximum. 

Thus, we need to ensure that we haven't found a local one.  

MLE of Normal distribution
===

Consider $$X_1, \ldots, X_n \stackrel{iid}{\sim} \text{Normal}(\theta, 1).$$

Show that the MLE is $\hat{\theta} =\bar{x}.$

MLE of Normal distribution
===

\begin{equation}
p(x_{1:n} \mid \theta) = (2 \pi)^{-n/2} \times \exp \{\frac{-1}{2} \sum_{i=1}^n (x_i - \theta )^2  \}
\end{equation}

Consider 

\begin{equation}
\log p(x_{1:n}) = -n/2 \log (2\pi) - \frac{1}{2} \sum_{i=1}^n (x_i - \theta )^2 
\end{equation}

MLE of Normal distribution
===

\begin{equation}
\frac{\partial p(x_{1:n} \mid \theta)}{\partial \theta} = \sum_{i=1}^n (x_i - \theta)
\end{equation}

This implies that 

$$\sum_i(x_i - \theta) = 0 \implies \hat{\theta} = \bar{x}.$$

MLE of Normal distribution
===

Consider 

$$\frac{\partial^2 p(x_{1:n} \mid \theta)}{\partial \theta^2} = -n < 0.$$
Thus, our solution is unique (and a global solution). 

Exercise
===

Show that 

$$\hat{\theta} = \bar{x}$$ is an unbiased estimator for $\theta.$

Proof. 

$$E[\hat{\theta}] = E[\bar{x}] = \frac{1}{n}\sum_iE[x_i] = \frac{1}{n} \sum_i \theta = \theta.$$

Thus, we have showed that the MLE is an unbiased estimator for $\theta.$





Normal-Normal model
===

Suppose that  $$X_1, \ldots, X_n \stackrel{iid}{\sim} \text{Normal}(\theta, 1),$$
where we now consider $$\theta \stackrel{ind}{\sim} \text{Normal}(\mu, \tau^2).$$


Let $\lambda = 1$ and $\lambda_o = 1/\tau^2.$

Recall that from module 3, $$\theta \mid x_{1:n} \sim N(M,L^{-1}),$$ where
$$L = n\lambda + \lambda_o$$
and $$M = \frac{n \lambda \bar{x} + \lambda_o\mu}{n\lambda + \lambda_o}.$$


Normal MLE
===

Observe that 

$$
M = \frac{n \lambda \bar{x} + \lambda_o\mu}{n\lambda + \lambda_o}
= \frac{n \lambda \hat{\theta} + \lambda_o\mu}{n\lambda + \lambda_o}
= \frac{n\lambda}{n\lambda + \lambda_o} \hat{\theta} + \frac{\lambda_o}{n\lambda + \lambda_o}\mu.
$$

Thus, we can write the posterior mean as a function of the MLE and the prior mean $\mu.$

Bernoulli-Bayes Estimation
===

\begin{align}
X_1, \ldots, X_n &\stackrel{iid}{\sim} \text{Bernoulli}(\theta). \\
\theta &\sim \text{Beta}(a,b)
\end{align}

Note that $Y = \sum_i X_i \sim Binomial(n, \theta).$

Exercise: The MLE for $\theta$ is $\bar{x} = y/n.$

Exercise: $$\theta \mid y \sim \text{Beta}(y + a, n - y + b).$$

Bernoulli-Bayes Estimation
===

Show that the posterior mean can be written as 

$$
E[\theta \mid y] =
\text{MLE} \times \frac{n}{a + b + n} + \text{priorMean} \times \frac{a+b}{a + b + n},
$$
where $\text{MLE}=\bar{x}$ and $\text{priorMean} = \frac{a}{a+b}.$

Bernoulli-Bayes Estimation
===

Proof:
$$
\begin{aligned}
E[\theta \mid y] &= \frac{y + a}{y + a +  n - y + b}  = \frac{y + a}{a +  n  + b} \\
&= \frac{y}{a + b + n} + \frac{a}{a + b + n} \\
&= 
\frac{y}{n} \times \frac{n}{a + b + n} + \frac{a}{a+b} \times \frac{a+b}{a + b + n} \\
&= 
\text{MLE} \times \frac{n}{a + b + n} + \text{priorMean} \times \frac{a+b}{a + b + n}
\end{aligned}
$$

Thus, we have written the posterior mean as a linear comboination of the MLE and prior mean with weights being determined by a,b, and n.

Evaluation of Estimators
===

How do we evaluate estimators? We often use the mean squared error. 

$$\text{MSE}(\hat{\theta}) = E_\theta [(\hat{\theta} - \theta)^2].$$

Observe that 

$$\text{MSE}(\hat{\theta}) = Var_\theta(\hat{\theta}) + E_\theta[(\hat{\theta} - \theta)^2] = Var_\theta(\hat{\theta}) + \text{Bias}_\theta (\hat{\theta}),$$

where the $$\text{Bias}_\theta (\hat{\theta}) = E_\theta(\hat{\theta}) - \theta.$$

For a more in depth treatment of MSE and bias, see Section 7.3.1, Casella and Berger, p. 330 - 334.

Binomial MLE
===
Let $$X_1, \ldots, X_n \stackrel{iid}{\sim} \text{Bernoulli}(\theta).$$

Show that the MLE is $\hat{\theta} =\bar{x}.$

Proof: Casella and Berger, Example 7.2.7, page 317-318.

Normal-Normal model
===
Suppose that  $$X_1, \ldots, X_n \stackrel{iid}{\sim} \text{Normal}(\theta, \sigma^2),$$
where $\theta, \sigma^2$ are both unknown. 

Show that $(\bar{x}, n^{-1} \sum_i (x_i - \bar{x})^2))$ are the MLE's for $(\theta, \sigma^2).$

Proof: Casella and Berger, Example 7.2.7, page 317-318.

Invariance property of MLE's
===

If $\hat{\theta}$ is the MLE of $\theta$, then for any function $g(\theta)$, the MLE of $g(\theta)$ 
is the MLE of $g(\hat{\theta}).$

Proof: Theorem 7.2.10, Casella and Berger, page 318.


Summary
===

- Unbiased Estimator
- MLEs
- Examples of MLE's
- MSE







